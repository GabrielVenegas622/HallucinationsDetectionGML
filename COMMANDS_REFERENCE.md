# Referencia de Comandos - Pipeline de Datos

## ğŸ”„ Flujo de Trabajo Completo

### 1. Pre-procesamiento Inicial
Convierte archivos .pkl/.pkl.gz originales a formato optimizado:

```bash
python src/preprocess_for_training.py \
    --data-pattern "traces_data/*.pkl*" \
    --scores-file traces_data/gt_*.csv \
    --output-dir preprocessed_data \
    --attn-threshold 0.0 \
    --score-threshold 0.5
```

**Resultado:**
- `preprocessed_data/lstm_solo/` - Datos para LSTM-solo
- `preprocessed_data/gnn/` - Datos para GNN-det+LSTM y GVAE+LSTM

---

### 2. DivisiÃ³n de Archivos (Opcional, para memoria limitada)
Divide archivos grandes en chunks pequeÃ±os:

```bash
# Para LSTM
python src/divide_and_conquer.py \
    --input-dir preprocessed_data/lstm_solo \
    --output-dir preprocessed_data/lstm_solo_split \
    --traces-per-part 50

# Para GNN
python src/divide_and_conquer.py \
    --input-dir preprocessed_data/gnn \
    --output-dir preprocessed_data/gnn_split \
    --traces-per-part 50
```

**Notas:**
- `--traces-per-part 50` divide archivos de 250 traces en 5 partes de 50
- Reduce uso de memoria durante entrenamiento
- Permite usar mÃ¡s `num_workers` en DataLoader

---

### 3. VerificaciÃ³n del Split
Verifica que la divisiÃ³n train/val/test sea correcta:

```bash
python src/test_data_split.py \
    --lstm-dir preprocessed_data/lstm_solo_split \
    --gnn-dir preprocessed_data/gnn_split
```

**Salida esperada:**
- Train: ~70% de archivos
- Val: ~15% de archivos
- Test: ~15% de archivos

---

### 4. Entrenamiento

#### 4.1 Entrenar todos los modelos
```bash
python src/baseline.py \
    --lstm-dir preprocessed_data/lstm_solo_split \
    --gnn-dir preprocessed_data/gnn_split \
    --batch-size 64 \
    --num-workers 4 \
    --epochs 50
```

#### 4.2 Entrenar desde GNN-det en adelante (skip LSTM-solo)
```bash
python src/baseline.py \
    --lstm-dir preprocessed_data/lstm_solo_split \
    --gnn-dir preprocessed_data/gnn_split \
    --skip-lstm \
    --batch-size 64 \
    --num-workers 4
```

#### 4.3 Entrenar solo un modelo especÃ­fico
```bash
# Solo LSTM
python src/baseline.py --lstm-dir ... --models lstm

# Solo GNN-det+LSTM
python src/baseline.py --gnn-dir ... --models gnn-det

# Solo GVAE+LSTM
python src/baseline.py --gnn-dir ... --models gvae
```

---

### 5. VisualizaciÃ³n de Resultados

```bash
python src/visualize_baseline.py
```

**Genera:**
- GrÃ¡ficas de Loss (train y validation)
- GrÃ¡ficas de AUROC (train y test)
- ComparaciÃ³n entre modelos (LSTM, GNN-det, GVAE)
- Archivos guardados en `visualizations/`

---

## ğŸ“Š Estructura de Archivos Resultantes

```
preprocessed_data/
â”œâ”€â”€ lstm_solo/                          # Original (puede ser grande)
â”‚   â””â”€â”€ preprocessed_*.pt
â”œâ”€â”€ lstm_solo_split/                    # Dividido (recomendado)
â”‚   â”œâ”€â”€ preprocessed_*_part0.pt
â”‚   â”œâ”€â”€ preprocessed_*_part1.pt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ gnn/                                # Original (puede ser grande)
â”‚   â””â”€â”€ preprocessed_*.pt
â””â”€â”€ gnn_split/                          # Dividido (recomendado)
    â”œâ”€â”€ preprocessed_*_part0.pt
    â””â”€â”€ ...

ablation_results/
â”œâ”€â”€ lstm_solo_TIMESTAMP.json            # Resultados LSTM
â”œâ”€â”€ gnn_det_lstm_TIMESTAMP.json         # Resultados GNN-det
â”œâ”€â”€ gvae_lstm_TIMESTAMP.json            # Resultados GVAE
â””â”€â”€ ablation_summary.json               # Resumen de todos

visualizations/
â”œâ”€â”€ baseline_losses.png
â”œâ”€â”€ baseline_auroc.png
â””â”€â”€ ...
```

---

## ğŸ” Troubleshooting

### Problema: "Out of Memory"
**SoluciÃ³n:** Usa archivos divididos y reduce batch_size
```bash
python src/divide_and_conquer.py --traces-per-part 25  # MÃ¡s pequeÃ±o
python src/baseline.py --batch-size 32  # Reducir batch
```

### Problema: "Train set vacÃ­o" (0 archivos)
**Causa:** Solo hay 1 archivo en la carpeta
**SoluciÃ³n:** Divide primero con `divide_and_conquer.py`

### Problema: Entrenamiento muy lento
**Causa:** num_workers=0 o archivos muy grandes
**SoluciÃ³n:**
```bash
python src/divide_and_conquer.py ...  # Dividir primero
python src/baseline.py --num-workers 4  # Usar paralelismo
```

### Problema: Test set tiene 183 archivos (desbalanceado)
**Causa:** Bug en versiÃ³n anterior (ya corregido)
**SoluciÃ³n:** Actualizar baseline.py (ya incluido en este fix)

---

## ğŸ“ Notas Importantes

1. **Orden recomendado**: preprocess â†’ divide â†’ verify â†’ train â†’ visualize
2. **Seed fijo**: Se usa `random.seed(42)` para reproducibilidad
3. **Split a nivel de archivos**: No de traces individuales
4. **Guardado automÃ¡tico**: Cada modelo se guarda al terminar su entrenamiento
5. **Threshold Ã³ptimo**: Se calcula en validaciÃ³n usando Youden's J statistic

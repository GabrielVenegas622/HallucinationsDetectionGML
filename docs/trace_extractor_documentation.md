# Documentaci√≥n: trace_extractor.py

## Descripci√≥n General

`trace_extractor.py` es un script de extracci√≥n de trazas de activaciones (hidden states) y atenciones (attention weights) de modelos de lenguaje tipo Transformer. Est√° dise√±ado espec√≠ficamente para la detecci√≥n de alucinaciones mediante el an√°lisis de la estructura interna del modelo durante la generaci√≥n de respuestas.

## Objetivo del Proyecto

Este script forma parte de un proyecto de investigaci√≥n que combina y extiende metodolog√≠as de papers como **CHARM** y **HalluShift** para detectar alucinaciones en LLMs. La innovaci√≥n principal consiste en:

1. **Modelar la generaci√≥n como un grafo**: Cada token es un nodo, las atenciones son arcos ponderados.
2. **Capturar el estado final completo**: En lugar de analizar paso a paso, se toma una "foto final" despu√©s de generar toda la respuesta.
3. **Incluir interacciones prompt-respuesta**: Las atenciones y activaciones incluyen tanto el prompt como la respuesta generada, permitiendo analizar c√≥mo el modelo usa el contexto.

---

## Arquitectura y Flujo de Trabajo

### 1. Inicializaci√≥n del Modelo

El script soporta modelos de Hugging Face con cuantizaci√≥n autom√°tica:

- **Pre-cuantizados** (e.g., `unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit`): Se cargan directamente.
- **Sin cuantizar**: Se aplica cuantizaci√≥n de 4-bit mediante BitsAndBytes para reducir el uso de VRAM.

**Par√°metros clave de cuantizaci√≥n:**
```python
BitsAndBytesConfig(
    load_in_4bit=True,                    # Reduce memoria ~75%
    bnb_4bit_compute_dtype=torch.float16, # Mantiene precisi√≥n en c√°lculos
    bnb_4bit_quant_type="nf4"             # NormalFloat4: √≥ptimo para LLMs
)
```

### 2. Carga de Datasets

Soporta dos datasets de benchmarking:

| Dataset | Split | Campo ID | Campo Pregunta | Uso |
|---------|-------|----------|----------------|-----|
| **TriviaQA** | `validation` (rc.nocontext) | `question_id` | `question` | Preguntas factuales con respuestas conocidas |
| **TruthfulQA** | `validation` (generation) | √çndice | `question` | Preguntas dise√±adas para inducir alucinaciones |

### 3. Extracci√≥n de Trazas

#### M√©todo: `extract_activations_and_attentions()`

Este es el n√∫cleo del script. Realiza lo siguiente:

**A. Construcci√≥n del Prompt**
```python
prompt_text = f"Answer the question concisely in one sentence.\n\nQuestion: {question}\nAnswer:"
```

**B. Generaci√≥n con Beam Search**
```python
generation_output = model.generate(
    num_beams=5,              # B√∫squeda de haz para diversidad
    do_sample=False,          # Generaci√≥n determinista
    max_new_tokens=64,        # L√≠mite de tokens
    early_stopping=True,      # Detener en EOS
    return_dict_in_generate=True,
    output_attentions=True,   # ‚úì Capturar matrices de atenci√≥n
    output_hidden_states=True # ‚úì Capturar activaciones
)
```

**C. Extracci√≥n del Estado Final**

En lugar de guardar estados intermedios (token 0, token 1, ..., token N), se captura **solo el estado final** que contiene toda la informaci√≥n de la secuencia completa:

```python
final_step_idx = len(generation_output.hidden_states) - 1
seq_len_total = len(prompt) + len(respuesta)
```

**Hidden States** (activaciones por capa):
- **Shape**: `[seq_len_total, hidden_dim]`
- **Contenido**: Activaciones de TODOS los tokens (prompt + respuesta) despu√©s del √∫ltimo forward pass.
- **Por qu√© es suficiente**: En Transformers, cada token tiene acceso a todos los anteriores mediante atenci√≥n. El estado final refleja la composici√≥n completa.

**Attentions** (pesos de atenci√≥n por capa):
- **Shape**: `[num_heads, seq_len_total, seq_len_total]`
- **Contenido**: Matriz completa de atenci√≥n donde `attn[i, j]` indica cu√°nto atiende el token `i` al token `j`.
- **Incluye**: 
  - Atenciones prompt ‚Üí prompt
  - Atenciones respuesta ‚Üí prompt (uso del contexto)
  - Atenciones respuesta ‚Üí respuesta (coherencia interna)

**D. Tokens Completos**

Para consistencia con las activaciones y atenciones, se guardan TODOS los tokens:

```python
all_tokens = generation_output.sequences[0, :seq_len_total]  # IDs
tokens_decoded = [tokenizer.decode([tid]) for tid in all_tokens]  # Strings
```

**Raz√≥n**: Los √≠ndices de `hidden_states[layer][i]` y `attentions[layer][:, i, j]` corresponden a posiciones en `tokens[i]`. Incluir solo los tokens de la respuesta romper√≠a esta correspondencia.

---

## Estructura de Datos Guardada

Cada trace (ejemplo procesado) contiene:

```python
{
    'question_id': str,              # ID √∫nico del dataset (e.g., "tc_123")
    'generated_answer_clean': str,   # Respuesta generada (solo texto, sin prompt)
    'hidden_states': List[ndarray],  # [num_layers] cada uno: [seq_len_total, hidden_dim]
    'attentions': List[ndarray],     # [num_layers] cada uno: [num_heads, seq_len_total, seq_len_total]
    'tokens': ndarray,               # [seq_len_total] IDs de tokens (prompt + respuesta)
    'tokens_decoded': List[str]      # [seq_len_total] Tokens como strings
}
```

### Ejemplo de dimensiones (Llama-2-7B):

- **Modelo**: 32 capas, 32 cabezas de atenci√≥n, dim=4096
- **Secuencia**: 50 tokens de prompt + 20 tokens de respuesta = 70 tokens totales

```python
trace['hidden_states'][0].shape  # (70, 4096)
trace['attentions'][0].shape     # (32, 70, 70)
trace['tokens'].shape            # (70,)
len(trace['tokens_decoded'])     # 70
```

---

## Gesti√≥n de Memoria: Sistema de Batches

Dado que cada trace puede pesar ~130 MB, procesar miles de ejemplos requiere estrategia:

### Par√°metros
- **BATCH_SIZE**: 500 traces por archivo
- **Memoria estimada**: 500 √ó 130 MB ‚âà 65 GB ‚Üí compresi√≥n pickle reduce a ~5 GB/batch

### Flujo
1. Procesar ejemplos secuencialmente
2. Acumular en lista `current_batch`
3. Al alcanzar 500 traces:
   - Guardar como `{modelo}_{dataset}_batch_{num:04d}.pkl`
   - Limpiar lista y liberar memoria (`gc.collect()`)
4. Repetir hasta terminar el dataset

### Nomenclatura de Archivos
```
llama2_chat_7B_triviaqa_batch_0000.pkl
llama2_chat_7B_triviaqa_batch_0001.pkl
llama2_chat_7B_truthfulqa_batch_0000.pkl
```

---

## Uso del Script

### Argumentos de L√≠nea de Comandos

```bash
python trace_extractor.py \
    --model llama2_chat_7B \
    --dataset triviaqa \
    --num-samples 1000
```

| Argumento | Tipo | Default | Descripci√≥n |
|-----------|------|---------|-------------|
| `--model` | str | `llama2_chat_7B` | ID del modelo (ver `HF_NAMES`) |
| `--dataset` | str | `triviaqa` | Dataset: `triviaqa` o `truthfulqa` |
| `--num-samples` | int | `None` | N√∫mero de muestras (None = todas) |

### Modelos Soportados (HF_NAMES)

Modificar el diccionario `HF_NAMES` para agregar modelos:

```python
HF_NAMES = {
    'qwen_2.5_6B': '__',  # Placeholder
    'llama2_chat_7B': 'meta-llama/Llama-2-7b-chat-hf',
}
```

---

## Consideraciones T√©cnicas

### 1. ¬øPor qu√© solo el estado final?

En modelos autoregresivos Transformer, generar token `t` requiere:
1. Codificar prompt + tokens [0, ..., t-1]
2. Aplicar atenci√≥n causal (solo tokens pasados)
3. Predecir token `t`

**Al final de la generaci√≥n**, el √∫ltimo forward pass contiene:
- Activaciones de TODOS los tokens procesados
- Atenciones acumuladas de toda la secuencia

**No se necesitan** estados intermedios porque el grafo final ya refleja todas las interacciones. Guardarlos solo aumentar√≠a el costo de almacenamiento sin aportar informaci√≥n adicional para el an√°lisis estructural.

### 2. Prompt vs. Respuesta en las Trazas

| Componente | Incluye Prompt | Raz√≥n |
|------------|----------------|-------|
| `hidden_states` | ‚úì S√≠ | Necesario para analizar c√≥mo las capas procesan contexto |
| `attentions` | ‚úì S√≠ | **Cr√≠tico**: Las atenciones respuesta‚Üíprompt revelan uso del contexto |
| `tokens` | ‚úì S√≠ | Consistencia: √≠ndices corresponden a posiciones en activaciones |
| `tokens_decoded` | ‚úì S√≠ | Visualizaci√≥n: etiquetas de nodos en el grafo |
| `generated_answer_clean` | ‚úó No | Solo la respuesta para validaci√≥n/evaluaci√≥n |

### 3. Detecci√≥n de Alucinaciones mediante Grafos

Cada capa genera un grafo:
- **Nodos**: Tokens (prompt + respuesta)
- **Atributos de nodo**: Activaciones `hidden_states[layer][token_idx]`
- **Arcos**: Pesos de atenci√≥n `attentions[layer][head, i, j]`

**Hip√≥tesis del proyecto**:
- Respuestas alucinadas muestran **menor atenci√≥n al contexto** (arcos d√©biles respuesta‚Üíprompt)
- **Patrones an√≥malos** en la evoluci√≥n de activaciones entre capas
- **Estructuras topol√≥gicas** distintivas (e.g., clustering, centralidad)

---

## Salida del Script

### Durante la Ejecuci√≥n
```
Cargando modelo: meta-llama/Llama-2-7b-chat-hf
‚öôÔ∏è  Aplicando cuantizaci√≥n de 4-bit con BitsAndBytes...
N√∫mero de capas del modelo: 32

Cargando dataset triviaqa...
N√∫mero de muestras a procesar: 5000
Tama√±o de batch: 500 traces por archivo
Archivos esperados: 10

--- Ejemplo 0 (Batch actual: 1/500) ---
Question ID: tc_123
Pregunta: What is the capital of France?...
Respuesta limpia: The capital of France is Paris....
Total de tokens (prompt + respuesta): 68
Primeros 5 tokens decodificados: ['Answer', ' the', ' question', ' con', 'cis']...
√öltimos 5 tokens decodificados: [' is', ' Paris', '.', '</s>']...

üíæ Guardando batch 0 en llama2_chat_7B_triviaqa_batch_0000.pkl...
   ‚úÖ Batch 0 guardado: 500 traces, 5234.56 MB
```

### Resumen Final
```
================================================================================
‚úÖ PROCESO COMPLETADO
================================================================================
Total de ejemplos procesados: 5000
Total de errores: 2
Total de batches guardados: 10
Directorio de salida: /path/to/traces_data

üìÅ Archivos generados:
   ‚Ä¢ llama2_chat_7B_triviaqa_batch_0000.pkl: 5234.56 MB
   ‚Ä¢ llama2_chat_7B_triviaqa_batch_0001.pkl: 5198.23 MB
   ...

üíæ Tama√±o total en disco: 52134.45 MB (50.91 GB)

--- An√°lisis del primer batch ---
Estructura de cada trace:
  - Campos guardados: ['question_id', 'generated_answer_clean', 'hidden_states', 'attentions', 'tokens', 'tokens_decoded']
  - N√∫mero de capas: 32
  - Total tokens en secuencia completa: 68
  - Shape de hidden state (capa 0): (68, 4096)
    ‚Üí seq_len=68 (prompt + respuesta), hidden_dim=4096
  - Shape de attention (capa 0): (32, 68, 68)
    ‚Üí num_heads=32, seq_len=68x68
```

---

## Optimizaciones y Limitaciones

### Optimizaciones Implementadas
1. **Cuantizaci√≥n 4-bit**: Reduce VRAM de ~28 GB a ~7 GB (Llama-2-7B)
2. **Batching**: Evita OOM al procesar datasets grandes
3. **Garbage collection**: Liberaci√≥n expl√≠cita de memoria entre batches
4. **Estado final √∫nico**: Ahorra ~90% de almacenamiento vs. guardar todos los pasos

### Limitaciones
1. **Tama√±o de archivos**: Batches de 5 GB requieren almacenamiento significativo
2. **Tiempo de procesamiento**: Beam search con `num_beams=5` es lento (~2-5 seg/ejemplo)
3. **VRAM**: Requiere GPU de al menos 8 GB para modelos 7B cuantizados
4. **Tokens largos**: Secuencias >512 tokens pueden exceder memoria

---

## Siguiente Paso: Construcci√≥n de Grafos

Los datos guardados permiten:

1. **Cargar batches**: 
   ```python
   import pickle
   with open('batch_0000.pkl', 'rb') as f:
       traces = pickle.load(f)
   ```

2. **Construir grafo por capa**:
   ```python
   import networkx as nx
   
   trace = traces[0]
   layer = 15  # Capa intermedia
   
   G = nx.DiGraph()
   
   # Nodos con activaciones
   for i, token in enumerate(trace['tokens_decoded']):
       G.add_node(i, 
                  label=token,
                  features=trace['hidden_states'][layer][i])
   
   # Arcos con atenciones (promedio sobre cabezas)
   attn = trace['attentions'][layer].mean(axis=0)  # [seq_len, seq_len]
   for i in range(len(attn)):
       for j in range(len(attn)):
           if attn[i, j] > 0.01:  # Umbral
               G.add_edge(j, i, weight=attn[i, j])
   ```

3. **An√°lisis GML**:
   - Graph Neural Networks (GCN, GAT)
   - M√©tricas topol√≥gicas (betweenness, PageRank)
   - Comparaci√≥n grafos alucinaci√≥n vs. verdad

---

## Dependencias

```txt
torch>=2.0.0
transformers>=4.35.0
datasets>=2.14.0
bitsandbytes>=0.41.0
accelerate>=0.24.0
huggingface_hub>=0.17.0
numpy>=1.24.0
tqdm>=4.65.0
```

---

## Referencias

- **CHARM**: Chen et al., "Characterizing Hallucination in LLMs via Attention Mechanisms"
- **HalluShift**: Wang et al., "Detecting Hallucinations through Attention Pattern Shifts"
- **Transformers**: Vaswani et al., "Attention Is All You Need" (2017)
- **BitsAndBytes**: Dettmers et al., "LLM.int8(): 8-bit Matrix Multiplication for Transformers" (2022)

---

## Autor y Licencia

**Proyecto**: Detecci√≥n de Alucinaciones con Graph Machine Learning  
**Curso**: IIC3641 - Universidad Santa Mar√≠a  
**A√±o**: 2025

Este c√≥digo es parte de un proyecto de investigaci√≥n acad√©mica.

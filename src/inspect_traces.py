"""
Script de ejemplo para cargar y explorar los traces extra√≠dos.
√ötil para verificar que los datos se guardaron correctamente.
"""

import pickle
import numpy as np
from pathlib import Path


def load_traces(traces_file):
    """Carga los traces desde el archivo pickle."""
    with open(traces_file, 'rb') as f:
        traces = pickle.load(f)
    return traces


def analyze_trace(trace, trace_idx=0):
    """Analiza un trace individual y muestra informaci√≥n detallada."""
    print(f"\n{'='*80}")
    print(f"AN√ÅLISIS DEL TRACE #{trace_idx}")
    print(f"{'='*80}")
    
    # Informaci√≥n b√°sica
    print(f"\nüìù Pregunta: {trace['question']}")
    print(f"\nüí¨ Respuesta Generada: {trace['generated_answer']}")
    
    if trace['ground_truth_answers']:
        print(f"\n‚úÖ Respuestas Correctas:")
        for i, ans in enumerate(trace['ground_truth_answers'][:3], 1):
            print(f"   {i}. {ans}")
    
    # Informaci√≥n de tokens
    num_tokens_total = len(trace['tokens'])
    num_tokens_generated = num_tokens_total - trace['prompt_length']
    print(f"\nüî¢ Tokens:")
    print(f"   - Total: {num_tokens_total}")
    print(f"   - Prompt: {trace['prompt_length']}")
    print(f"   - Generados: {num_tokens_generated}")
    
    # Informaci√≥n de capas
    num_layers = trace['num_layers']
    print(f"\nüèóÔ∏è  Arquitectura:")
    print(f"   - N√∫mero de capas: {num_layers}")
    
    # An√°lisis de hidden states
    print(f"\nüß† Hidden States:")
    print(f"   - Estructura: {len(trace['hidden_states'])} capas")
    if trace['hidden_states']:
        # Analizar primera capa
        first_layer = trace['hidden_states'][0]
        print(f"   - Tokens capturados por capa: {len(first_layer)}")
        if first_layer:
            first_state = first_layer[0]
            print(f"   - Shape de cada estado: {first_state.shape}")
            print(f"   - Dimensi√≥n oculta: {first_state.shape[-1]}")
    
    # An√°lisis de atenciones
    print(f"\nüëÅÔ∏è  Attention Matrices:")
    print(f"   - Estructura: {len(trace['attentions'])} capas")
    if trace['attentions']:
        # Analizar primera capa
        first_layer_attn = trace['attentions'][0]
        print(f"   - Tokens capturados por capa: {len(first_layer_attn)}")
        if first_layer_attn:
            first_attn = first_layer_attn[0]
            print(f"   - Shape de cada matriz: {first_attn.shape}")
            batch, num_heads, seq_len, _ = first_attn.shape
            print(f"   - N√∫mero de cabezas: {num_heads}")
            print(f"   - Secuencia m√°xima: {seq_len}")
            
            # Estad√≠sticas de la primera matriz de atenci√≥n
            print(f"\n   üìä Estad√≠sticas (primera matriz, promedio sobre cabezas):")
            avg_attn = first_attn[0].mean(axis=0)  # Promediar sobre cabezas
            print(f"      - Media: {avg_attn.mean():.4f}")
            print(f"      - Std: {avg_attn.std():.4f}")
            print(f"      - Min: {avg_attn.min():.4f}")
            print(f"      - Max: {avg_attn.max():.4f}")


def analyze_dataset_statistics(traces):
    """Analiza estad√≠sticas globales del dataset."""
    print(f"\n{'='*80}")
    print(f"ESTAD√çSTICAS DEL DATASET COMPLETO")
    print(f"{'='*80}")
    
    num_traces = len(traces)
    print(f"\nüìä Tama√±o del dataset: {num_traces} ejemplos")
    
    # Longitudes de respuestas
    answer_lengths = []
    for trace in traces:
        num_generated = len(trace['tokens']) - trace['prompt_length']
        answer_lengths.append(num_generated)
    
    print(f"\nüìè Longitud de respuestas generadas:")
    print(f"   - Media: {np.mean(answer_lengths):.2f} tokens")
    print(f"   - Mediana: {np.median(answer_lengths):.2f} tokens")
    print(f"   - Min: {np.min(answer_lengths)} tokens")
    print(f"   - Max: {np.max(answer_lengths)} tokens")
    print(f"   - Std: {np.std(answer_lengths):.2f} tokens")
    
    # Verificar consistencia
    num_layers_list = [trace['num_layers'] for trace in traces]
    unique_layers = set(num_layers_list)
    print(f"\nüèóÔ∏è  Capas por modelo: {unique_layers}")
    
    # Tama√±o en memoria
    import sys
    size_mb = sys.getsizeof(pickle.dumps(traces)) / (1024 * 1024)
    print(f"\nüíæ Tama√±o estimado en memoria: {size_mb:.2f} MB")


def main():
    # Buscar archivos de traces
    traces_dir = Path("./traces_data")
    
    if not traces_dir.exists():
        print(f"‚ùå No se encontr√≥ el directorio {traces_dir}")
        print("   Ejecuta primero trace_extractor.py")
        return
    
    # Buscar archivos pickle (batch y archivos antiguos)
    batch_files = sorted(traces_dir.glob("trivia_qa_traces_batch_*.pkl"))
    old_files = list(traces_dir.glob("trivia_qa_traces_*.pkl"))
    old_files = [f for f in old_files if "batch" not in f.name]
    
    if not batch_files and not old_files:
        print(f"‚ùå No se encontraron archivos .pkl en {traces_dir}")
        return
    
    # Mostrar informaci√≥n sobre archivos encontrados
    print(f"{'='*80}")
    print("ARCHIVOS DE TRACES ENCONTRADOS")
    print(f"{'='*80}\n")
    
    if batch_files:
        print(f"‚úÖ Archivos en batch: {len(batch_files)}")
        total_size = 0
        for f in batch_files:
            size_mb = f.stat().st_size / (1024 * 1024)
            total_size += size_mb
            print(f"   ‚Ä¢ {f.name}: {size_mb:.2f} MB")
        print(f"\nüíæ Tama√±o total de batches: {total_size:.2f} MB ({total_size/1024:.2f} GB)")
    
    if old_files:
        print(f"\nüì¶ Archivos individuales (formato antiguo): {len(old_files)}")
        for f in old_files:
            size_mb = f.stat().st_size / (1024 * 1024)
            print(f"   ‚Ä¢ {f.name}: {size_mb:.2f} MB")
    
    # Cargar y analizar batches
    if batch_files:
        print(f"\n{'='*80}")
        print("AN√ÅLISIS DE BATCHES")
        print(f"{'='*80}\n")
        
        all_traces_count = 0
        answer_lengths = []
        
        # Analizar cada batch
        for batch_idx, batch_file in enumerate(batch_files):
            print(f"üìÇ Cargando batch {batch_idx}: {batch_file.name}...")
            
            try:
                with open(batch_file, 'rb') as f:
                    traces = load_traces(batch_file)
                
                num_traces = len(traces)
                all_traces_count += num_traces
                
                print(f"   ‚úÖ {num_traces} traces en este batch")
                
                # Recopilar estad√≠sticas
                for trace in traces:
                    num_generated = len(trace['tokens']) - trace['prompt_length']
                    answer_lengths.append(num_generated)
                
                # Mostrar ejemplo del primer batch
                if batch_idx == 0 and traces:
                    analyze_trace(traces[0], 0)
                
            except Exception as e:
                print(f"   ‚ùå Error cargando batch {batch_idx}: {e}")
        
        # Estad√≠sticas globales
        print(f"\n{'='*80}")
        print("ESTAD√çSTICAS GLOBALES DEL DATASET")
        print(f"{'='*80}\n")
        
        print(f"üìä Total de traces en todos los batches: {all_traces_count}")
        
        if answer_lengths:
            print(f"\nüìè Longitud de respuestas generadas:")
            print(f"   - Media: {np.mean(answer_lengths):.2f} tokens")
            print(f"   - Mediana: {np.median(answer_lengths):.2f} tokens")
            print(f"   - Min: {np.min(answer_lengths)} tokens")
            print(f"   - Max: {np.max(answer_lengths)} tokens")
            print(f"   - Std: {np.std(answer_lengths):.2f} tokens")
        
        # Mostrar algunos ejemplos de diferentes batches
        print(f"\n{'='*80}")
        print("EJEMPLOS DE DIFERENTES BATCHES")
        print(f"{'='*80}")
        
        for batch_idx in [0, len(batch_files)//2, len(batch_files)-1]:
            if batch_idx < len(batch_files):
                print(f"\n--- Ejemplo del batch {batch_idx} ---")
                with open(batch_files[batch_idx], 'rb') as f:
                    traces = pickle.load(f)
                if traces:
                    trace = traces[0]
                    num_gen = len(trace['tokens']) - trace['prompt_length']
                    print(f"  Q: {trace['question'][:60]}...")
                    print(f"  A: {trace['generated_answer'][:60]}...")
                    print(f"  Tokens generados: {num_gen}")
                    print(f"  Batch number: {trace.get('batch_number', 'N/A')}")
                    print(f"  Global ID: {trace.get('global_example_id', 'N/A')}")
        
    # Si hay archivos antiguos, tambi√©n analizarlos
    elif old_files:
        traces_file = old_files[0]
        print(f"\nüìÇ Cargando: {traces_file}")
        
        try:
            traces = load_traces(traces_file)
            print(f"‚úÖ Cargado exitosamente: {len(traces)} traces")
            analyze_dataset_statistics(traces)
            if traces:
                analyze_trace(traces[0], 0)
        except Exception as e:
            print(f"‚ùå Error al cargar los traces: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n{'='*80}")
    print("‚úÖ An√°lisis completado")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()

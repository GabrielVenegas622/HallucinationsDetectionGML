# GuÃ­a de VisualizaciÃ³n de Grafos de AtenciÃ³n

## ğŸ“Š DescripciÃ³n

El script `visualize_attention_graph.py` genera visualizaciones de los grafos de atenciÃ³n que se construyen a partir de los traces. Muestra exactamente cÃ³mo se ven los datos que alimentan el modelo de detecciÃ³n de alucinaciones.

## ğŸ¨ Tipos de Visualizaciones

### 1. Grafo de AtenciÃ³n Individual
Visualiza una capa especÃ­fica mostrando:
- **Nodos**: Tokens con sus etiquetas
- **Arcos**: Conexiones de atenciÃ³n (coloreadas por intensidad)
- **InformaciÃ³n**: Prompt, respuesta y estadÃ­sticas

### 2. ComparaciÃ³n Entre Capas
Muestra cÃ³mo evoluciona el patrÃ³n de atenciÃ³n a travÃ©s de las capas (primera, media, Ãºltima).

### 3. Heatmap de Matriz de AtenciÃ³n
VisualizaciÃ³n de la matriz completa de atenciÃ³n promediada sobre las cabezas.

## ğŸš€ Uso BÃ¡sico

### VisualizaciÃ³n Simple de Una Capa

```bash
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --trace-idx 0 \
    --layer-idx 15 \
    --output-dir ./visualizations
```

**Resultado:** Un grafo de la capa 15 del primer trace.

### VisualizaciÃ³n Completa (3 Tipos)

```bash
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --trace-idx 0 \
    --layer-idx 15 \
    --compare-layers \
    --create-heatmap \
    --output-dir ./visualizations
```

**Resultado:** 
- `graph_layer_15.png` - Grafo individual
- `layerwise_comparison.png` - ComparaciÃ³n entre capas
- `attention_heatmap_layer_15.png` - Heatmap de atenciÃ³n

## ğŸ“‹ ParÃ¡metros Completos

| ParÃ¡metro | DescripciÃ³n | Default | Ejemplo |
|-----------|-------------|---------|---------|
| `--data-pattern` | PatrÃ³n glob para .pkl | **Requerido** | `"traces_data/*.pkl"` |
| `--trace-idx` | Ãndice del trace | `0` | `5` |
| `--layer-idx` | Capa a visualizar | `15` | `0`, `31` |
| `--attn-threshold` | Umbral para arcos | `0.01` | `0.05` |
| `--output-dir` | Directorio de salida | `./visualizations` | `./figuras` |
| `--layout` | Layout del grafo | `spring` | `circular`, `kamada_kawai` |
| `--max-nodes` | MÃ¡x. nodos a mostrar | `50` | `100` |
| `--compare-layers` | ComparaciÃ³n entre capas | `False` | Flag (activa) |
| `--create-heatmap` | Crear heatmap | `False` | Flag (activa) |

## ğŸ¯ Ejemplos de Uso

### Ejemplo 1: Explorar Diferentes Capas

```bash
# Capa inicial (0)
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --layer-idx 0 \
    --output-dir ./visualizations/layer_0

# Capa media (15)
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --layer-idx 15 \
    --output-dir ./visualizations/layer_15

# Capa final (31)
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --layer-idx 31 \
    --output-dir ./visualizations/layer_31
```

### Ejemplo 2: Diferentes Layouts

```bash
# Layout spring (default - distribuido)
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --layout spring

# Layout circular (tokens en cÃ­rculo)
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --layout circular

# Layout secuencial (tokens en lÃ­nea)
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --layout sequential
```

### Ejemplo 3: Ajustar Threshold de AtenciÃ³n

```bash
# Threshold bajo (mÃ¡s arcos, mÃ¡s conexiones)
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --attn-threshold 0.001

# Threshold alto (menos arcos, solo conexiones fuertes)
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --attn-threshold 0.05
```

### Ejemplo 4: Visualizar MÃºltiples Traces

```bash
# Crear visualizaciones de los primeros 5 traces
for i in {0..4}; do
    python src/visualize_attention_graph.py \
        --data-pattern "traces_data/*.pkl" \
        --trace-idx $i \
        --layer-idx 15 \
        --compare-layers \
        --create-heatmap \
        --output-dir ./visualizations/trace_$i
done
```

## ğŸ¨ InterpretaciÃ³n de las Visualizaciones

### Grafo de AtenciÃ³n

**Nodos (Tokens):**
- Color azul claro
- Etiqueta = texto del token
- TamaÃ±o = 800 (fijo)

**Arcos (AtenciÃ³n):**
- Color: Azul claro â†’ Azul â†’ Naranja â†’ Rojo
  - Azul claro: AtenciÃ³n dÃ©bil
  - Rojo: AtenciÃ³n fuerte
- DirecciÃ³n: Flecha indica `source â†’ target`
- Grosor: Fijo (2.0)
- Solo se muestran arcos > threshold

**Colorbar:**
- Escala indica el rango de pesos de atenciÃ³n
- Min: ConexiÃ³n mÃ¡s dÃ©bil mostrada
- Max: ConexiÃ³n mÃ¡s fuerte

### ComparaciÃ³n Entre Capas

Muestra 3 grafos lado a lado:
- **Capa 0** (inicial): Patrones de atenciÃ³n tempranos
- **Capa media** (15-16): Procesamiento intermedio  
- **Capa final** (31): Patrones de atenciÃ³n refinados

**Observaciones tÃ­picas:**
- Capas iniciales: AtenciÃ³n mÃ¡s dispersa
- Capas finales: AtenciÃ³n mÃ¡s concentrada en tokens relevantes

### Heatmap de AtenciÃ³n

**Ejes:**
- Eje X: Key tokens (a quÃ© atiende)
- Eje Y: Query tokens (quiÃ©n atiende)

**Colores:**
- Amarillo claro: Poca atenciÃ³n
- Naranja: AtenciÃ³n moderada
- Rojo oscuro: AtenciÃ³n alta

**Patrones comunes:**
- Diagonal: Auto-atenciÃ³n (token atiende a sÃ­ mismo)
- Bloques: Grupos de tokens relacionados
- Columnas destacadas: Tokens importantes (ej: palabras clave del prompt)

## ğŸ“Š Salida del Script

```
================================================================================
VISUALIZACIÃ“N DE GRAFOS DE ATENCIÃ“N
================================================================================

ğŸ“‚ Cargando trace 0 de traces_data/*.pkl...
âœ“ Trace cargado: qb_3343
  NÃºmero de capas: 32
  Tokens: 34
  Respuesta: Qatar....

ğŸ¨ Generando visualizaciÃ³n de capa 15...
âš ï¸  El grafo tiene 34 nodos. Mostrando solo los primeros 50.
âœ… GrÃ¡fico guardado en: visualizations/graph_layer_15.png

ğŸ¨ Generando comparaciÃ³n entre capas...
âœ… ComparaciÃ³n guardada en: visualizations/layerwise_comparison.png

ğŸ¨ Generando heatmap de atenciÃ³n de capa 15...
âœ… Heatmap guardado en: visualizations/attention_heatmap_layer_15.png

================================================================================
âœ… VISUALIZACIÃ“N COMPLETADA
ğŸ“ Archivos guardados en: visualizations
================================================================================
```

## ğŸ”§ Troubleshooting

### Problema: "No se encontraron archivos"
```bash
# Verificar el patrÃ³n
ls traces_data/*.pkl

# Ajustar patrÃ³n si es necesario
python src/visualize_attention_graph.py \
    --data-pattern "/ruta/completa/traces_data/*.pkl"
```

### Problema: "trace_idx fuera de rango"
```bash
# Ver cuÃ¡ntos traces hay
python -c "import pickle; print(len(pickle.load(open('traces_data/batch_0001.pkl', 'rb'))))"

# Usar Ã­ndice vÃ¡lido
python src/visualize_attention_graph.py ... --trace-idx 0
```

### Problema: "Grafo muy grande"
```bash
# Reducir nÃºmero de nodos mostrados
python src/visualize_attention_graph.py \
    ... \
    --max-nodes 30
```

### Problema: "Muy pocos arcos"
```bash
# Reducir threshold
python src/visualize_attention_graph.py \
    ... \
    --attn-threshold 0.001
```

## ğŸ’¡ Consejos

1. **Para presentaciones:** Usar `--layout circular` (mÃ¡s limpio visualmente)

2. **Para anÃ¡lisis detallado:** Usar `--create-heatmap` (muestra patrones completos)

3. **Para comparaciones:** Usar `--compare-layers` (evoluciÃ³n a travÃ©s de capas)

4. **Grafos grandes:** Combinar `--max-nodes 30` con `--attn-threshold 0.05`

5. **Alta resoluciÃ³n:** Las imÃ¡genes se guardan a 300 DPI, ideales para papers

## ğŸ“š Dependencias

```bash
pip install networkx matplotlib numpy
```

## ğŸ¯ Casos de Uso

### Para Papers/Presentaciones
```bash
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --trace-idx 0 \
    --layer-idx 15 \
    --layout circular \
    --compare-layers \
    --create-heatmap \
    --max-nodes 40 \
    --output-dir ./paper_figures
```

### Para Debugging
```bash
python src/visualize_attention_graph.py \
    --data-pattern "traces_data/*.pkl" \
    --trace-idx 0 \
    --layer-idx 0 \
    --attn-threshold 0.001 \
    --output-dir ./debug
```

### Para ExploraciÃ³n
```bash
# Ver mÃºltiples capas de un trace
for layer in 0 5 10 15 20 25 31; do
    python src/visualize_attention_graph.py \
        --data-pattern "traces_data/*.pkl" \
        --layer-idx $layer \
        --output-dir ./exploration/layer_$layer
done
```

---
**Tip:** Combina con `inspect_trace_structure.py` para primero entender tus datos, luego visualizarlos.
